{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4085d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from load_data import DataLoader\n",
    "from model import DeepIRTModel\n",
    "from run import run_model\n",
    "from utils import getLogger\n",
    "from configs import ModelConfigFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa8349f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51f96768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-13 17:39:03,281 - Deep-IRT-model - INFO - Cross Validation 1\n",
      "2023-08-13 17:39:03,283 - Deep-IRT-model - INFO - Initializing Placeholder\n",
      "2023-08-13 17:39:03,286 - Deep-IRT-model - INFO - Initializing Key and Value Memory\n",
      "2023-08-13 17:39:03,301 - Deep-IRT-model - INFO - Initializing Q and QA Embedding\n",
      "2023-08-13 17:39:03,317 - Deep-IRT-model - INFO - Initializing Embedding Lookup\n",
      "2023-08-13 17:39:03,322 - Deep-IRT-model - INFO - Initializing Influence Procedure\n",
      "D:\\Anaconda Python\\envs\\data_analytics\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "2023-08-13 17:39:03,530 - Deep-IRT-model - INFO - Initializing Loss Function\n",
      "2023-08-13 17:39:04,680 - Deep-IRT-model - INFO - Memory/key_memory_matrix:0 (float32 10x10) [100, bytes: 400]\n",
      "Memory/value_memory_matrix:0 (float32 10x20) [200, bytes: 800]\n",
      "Embedding/q_embed:0 (float32 12444x10) [124440, bytes: 497760]\n",
      "Embedding/qa_embed:0 (float32 24887x20) [497740, bytes: 1990960]\n",
      "DKVMN-ValueHead/EraseOperation/weights:0 (float32 20x20) [400, bytes: 1600]\n",
      "DKVMN-ValueHead/EraseOperation/biases:0 (float32 20) [20, bytes: 80]\n",
      "DKVMN-ValueHead/AddOperation/weights:0 (float32 20x20) [400, bytes: 1600]\n",
      "DKVMN-ValueHead/AddOperation/biases:0 (float32 20) [20, bytes: 80]\n",
      "SummaryOperation/weights:0 (float32 30x10) [300, bytes: 1200]\n",
      "SummaryOperation/biases:0 (float32 10) [10, bytes: 40]\n",
      "StudentAbilityOutputLayer/weights:0 (float32 10x1) [10, bytes: 40]\n",
      "StudentAbilityOutputLayer/biases:0 (float32 1) [1, bytes: 4]\n",
      "QuestionDifficultyOutputLayer/weights:0 (float32 10x1) [10, bytes: 40]\n",
      "QuestionDifficultyOutputLayer/biases:0 (float32 1) [1, bytes: 4]\n",
      "Total size of variables: 623652 \n",
      "Total bytes of variables: 2494608 \n",
      "\n",
      "2023-08-13 17:39:04,764 - Deep-IRT-model - INFO - Reading ./data/mydata\\my_data_train0.csv and ./data/mydata\\my_data_valid0.csv\n",
      "2023-08-13 17:39:05,330 - Deep-IRT-model - INFO - \n",
      "[Epoch 1/1] Training result:      AUC: 33.33%\t Acc: 37.50%\t Loss: 14.3912\n",
      "[Epoch 1/1] Validation result:    AUC: 58.33%\t Acc: 62.50%\t Loss: 8.6347\n",
      "2023-08-13 17:39:05,331 - Deep-IRT-model - INFO - Model improved.\n",
      "2023-08-13 17:39:05,332 - Deep-IRT-model - INFO - Best result at epoch 1: AUC: 58.33\t Accuracy: 62.50\t Loss: 8.6347\t Valid ability: [ 0.17799792  0.09599354  0.0332391   0.04857728 -0.01726536 -0.04803391\n",
      "  0.08205722 -0.01927519]\n",
      "2023-08-13 17:39:05,342 - Deep-IRT-model - INFO - Cross Validation 2\n",
      "2023-08-13 17:39:05,345 - Deep-IRT-model - INFO - Initializing Placeholder\n",
      "2023-08-13 17:39:05,349 - Deep-IRT-model - INFO - Initializing Key and Value Memory\n",
      "2023-08-13 17:39:05,377 - Deep-IRT-model - INFO - Initializing Q and QA Embedding\n",
      "2023-08-13 17:39:05,390 - Deep-IRT-model - INFO - Initializing Embedding Lookup\n",
      "2023-08-13 17:39:05,398 - Deep-IRT-model - INFO - Initializing Influence Procedure\n",
      "D:\\Anaconda Python\\envs\\data_analytics\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "2023-08-13 17:39:05,603 - Deep-IRT-model - INFO - Initializing Loss Function\n",
      "2023-08-13 17:39:06,386 - Deep-IRT-model - INFO - Memory/key_memory_matrix:0 (float32 10x10) [100, bytes: 400]\n",
      "Memory/value_memory_matrix:0 (float32 10x20) [200, bytes: 800]\n",
      "Embedding/q_embed:0 (float32 12444x10) [124440, bytes: 497760]\n",
      "Embedding/qa_embed:0 (float32 24887x20) [497740, bytes: 1990960]\n",
      "DKVMN-ValueHead/EraseOperation/weights:0 (float32 20x20) [400, bytes: 1600]\n",
      "DKVMN-ValueHead/EraseOperation/biases:0 (float32 20) [20, bytes: 80]\n",
      "DKVMN-ValueHead/AddOperation/weights:0 (float32 20x20) [400, bytes: 1600]\n",
      "DKVMN-ValueHead/AddOperation/biases:0 (float32 20) [20, bytes: 80]\n",
      "SummaryOperation/weights:0 (float32 30x10) [300, bytes: 1200]\n",
      "SummaryOperation/biases:0 (float32 10) [10, bytes: 40]\n",
      "StudentAbilityOutputLayer/weights:0 (float32 10x1) [10, bytes: 40]\n",
      "StudentAbilityOutputLayer/biases:0 (float32 1) [1, bytes: 4]\n",
      "QuestionDifficultyOutputLayer/weights:0 (float32 10x1) [10, bytes: 40]\n",
      "QuestionDifficultyOutputLayer/biases:0 (float32 1) [1, bytes: 4]\n",
      "Total size of variables: 623652 \n",
      "Total bytes of variables: 2494608 \n",
      "\n",
      "2023-08-13 17:39:06,469 - Deep-IRT-model - INFO - Reading ./data/mydata\\my_data_train1.csv and ./data/mydata\\my_data_valid1.csv\n",
      "2023-08-13 17:39:06,943 - Deep-IRT-model - INFO - \n",
      "[Epoch 1/1] Training result:      AUC: 58.33%\t Acc: 50.00%\t Loss: 11.5129\n",
      "[Epoch 1/1] Validation result:    AUC: 16.67%\t Acc: 25.00%\t Loss: 17.2694\n",
      "2023-08-13 17:39:06,945 - Deep-IRT-model - INFO - Model improved.\n",
      "2023-08-13 17:39:06,946 - Deep-IRT-model - INFO - Best result at epoch 1: AUC: 16.67\t Accuracy: 25.00\t Loss: 17.2694\t Valid ability: [-0.18632653 -0.06553315 -0.18330035  0.05850279 -0.08913452 -0.02749979\n",
      "  0.00800037 -0.00443278]\n",
      "2023-08-13 17:39:06,955 - Deep-IRT-model - INFO - Cross Validation Result:\n",
      "AUC: 37.50 +/- 20.83\n",
      "Accuracy: 43.75 +/- 18.75\n",
      "Loss: 12.95 +/- 4.32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set logger\n",
    "logger = getLogger('Deep-IRT-model')\n",
    "\n",
    "# # argument parser\n",
    "# parser = argparse.ArgumentParser()\n",
    "# # dataset can be assist2009, assist2015, statics2011, synthetic, fsai\n",
    "# parser.add_argument('--dataset', default='assist2009', \n",
    "#                     help=\"'assist2009', 'assist2015', 'statics2011', 'synthetic', 'fsai'\")\n",
    "                    \n",
    "# parser.add_argument('--save', type=bool, default=False)\n",
    "# parser.add_argument('--cpu', type=bool, default=False)\n",
    "# parser.add_argument('--n_epochs', type=int, default=None)\n",
    "# parser.add_argument('--batch_size', type=int, default=None)\n",
    "# parser.add_argument('--train', type=bool, default=None)\n",
    "# parser.add_argument('--show', type=bool, default=None)    \n",
    "# parser.add_argument('--learning_rate', type=float, default=None)\n",
    "# parser.add_argument('--max_grad_norm', type=float, default=None)\n",
    "# parser.add_argument('--use_ogive_model', type=bool, default=False)\n",
    "\n",
    "# # parameter for the dataset\n",
    "# parser.add_argument('--seq_len', type=int, default=None)\n",
    "# parser.add_argument('--n_questions', type=int, default=None)\n",
    "# parser.add_argument('--data_dir', type=str, default=None)\n",
    "# parser.add_argument('--data_name', type=str, default=None)\n",
    "\n",
    "# # parameter for the DKVMN model\n",
    "# parser.add_argument('--memory_size', type=int, default=None)\n",
    "# parser.add_argument('--key_memory_state_dim', type=int, default=None)\n",
    "# parser.add_argument('--value_memory_state_dim', type=int, default=None)\n",
    "# parser.add_argument('--summary_vector_output_dim', type=int, default=None)\n",
    "\n",
    "# _args = parser.parse_args()\n",
    "# args = ModelConfigFactory.create_model_config(_args)\n",
    "# logger.info(\"Model Config: {}\".format(args))\n",
    "class argus:\n",
    "    def __init__(self):\n",
    "        # training setting\n",
    "        self.save= False\n",
    "        self.cpu= True\n",
    "        self.n_epochs= 1\n",
    "        self.batch_size= 32\n",
    "        self.train= True\n",
    "        self.show= True\n",
    "        self.learning_rate= 0.003\n",
    "        self.max_grad_norm= 10.0\n",
    "        self.use_ogive_model= False\n",
    "        # dataset param\n",
    "        self.seq_len= 200\n",
    "        self.n_questions= 110\n",
    "        self.data_dir= './data/assist2009_updated'\n",
    "        self.data_name= 'assist2009_updated'\n",
    "        # DKVMN param\n",
    "        self.memory_size= 50\n",
    "        self.key_memory_state_dim= 50\n",
    "        self.value_memory_state_dim= 100\n",
    "        self.summary_vector_output_dim= 50\n",
    "        # parameter for the SA Network and KCD network\n",
    "        self.student_ability_layer_structure= None\n",
    "        self.question_difficulty_layer_structure= None\n",
    "        self.discimination_power_layer_structure= None\n",
    "        # dataset save result\n",
    "        self.checkpoint_dir= './Model/checkpoints/'\n",
    "        self.result_log_dir= '/Model/results/'\n",
    "        self.tensorboard_dir= '/Model/tensorboard/' \n",
    "\n",
    "class argus:\n",
    "    def __init__(self):\n",
    "        # training setting\n",
    "        self.dataset = 'my_data'\n",
    "        self.save= False\n",
    "        self.cpu= True\n",
    "        self.n_epochs= 1\n",
    "        self.batch_size= 1\n",
    "        self.train= True\n",
    "        self.show= True\n",
    "        self.learning_rate= 0.003\n",
    "        self.max_grad_norm= 10.0\n",
    "        self.use_ogive_model= False\n",
    "        # dataset param\n",
    "        self.seq_len= 4\n",
    "        self.n_questions= 12443\n",
    "        self.data_dir= './data/mydata'\n",
    "        self.data_name= 'my_data'\n",
    "        # DKVMN param\n",
    "        self.memory_size= 10\n",
    "        self.key_memory_state_dim= 10\n",
    "        self.value_memory_state_dim= 20\n",
    "        self.summary_vector_output_dim= 10\n",
    "        # parameter for the SA Network and KCD network\n",
    "        self.student_ability_layer_structure= None\n",
    "        self.question_difficulty_layer_structure= None\n",
    "        self.discimination_power_layer_structure= None\n",
    "        # dataset save result\n",
    "        self.checkpoint_dir= './Model/checkpoints/'\n",
    "        self.result_log_dir= '/Model/results/'\n",
    "        self.tensorboard_dir= '/Model/tensorboard/'         \n",
    "        \n",
    "        \n",
    "args = argus()\n",
    "\n",
    "# print(args.save)      \n",
    "# logger.info(\"Model Config: {}\".format(args))\n",
    "        \n",
    "# create directory\n",
    "for directory in [args.checkpoint_dir, args.result_log_dir, args.tensorboard_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def train(model, train_q_data, train_qa_data, \n",
    "            valid_q_data, valid_qa_data, result_log_path, args):\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    best_loss = 1e6\n",
    "    best_acc = 0.0\n",
    "    best_auc = 0.0\n",
    "    best_epoch = 0.0\n",
    "\n",
    "    with open(result_log_path, 'w') as f:\n",
    "        result_msg = \"{},{},{},{},{},{},{}\\n\".format(\n",
    "            'epoch', \n",
    "            'train_auc', 'train_accuracy', 'train_loss',\n",
    "            'valid_auc', 'valid_accuracy', 'valid_loss'\n",
    "        )\n",
    "        f.write(result_msg)\n",
    "    for epoch in range(args.n_epochs):\n",
    "        \n",
    "        train_loss, train_accuracy, train_auc, train_ability = run_model(\n",
    "            model, args, train_q_data, train_qa_data, mode='train'\n",
    "        )\n",
    "        valid_loss, valid_accuracy, valid_auc, valid_ability = run_model(\n",
    "            model, args, valid_q_data, valid_qa_data, mode='valid'\n",
    "        )\n",
    "\n",
    "        # add to log\n",
    "        msg = \"\\n[Epoch {}/{}] Training result:      AUC: {:.2f}%\\t Acc: {:.2f}%\\t Loss: {:.4f}\".format(\n",
    "            epoch+1, args.n_epochs, train_auc*100, train_accuracy*100, train_loss\n",
    "        )\n",
    "        msg += \"\\n[Epoch {}/{}] Validation result:    AUC: {:.2f}%\\t Acc: {:.2f}%\\t Loss: {:.4f}\".format(\n",
    "            epoch+1, args.n_epochs, valid_auc*100, valid_accuracy*100, valid_loss\n",
    "        )\n",
    "        logger.info(msg)\n",
    "\n",
    "        # write epoch result\n",
    "        with open(result_log_path, 'a') as f:\n",
    "            result_msg = \"{},{},{},{},{},{},{}\\n\".format(\n",
    "                epoch, \n",
    "                train_auc, train_accuracy, train_loss,\n",
    "                valid_auc, valid_accuracy, valid_loss\n",
    "            )\n",
    "            f.write(result_msg)\n",
    "\n",
    "        # add to tensorboard\n",
    "        tf_summary = tf.compat.v1.Summary(\n",
    "            value=[\n",
    "                tf.compat.v1.Summary.Value(tag=\"train_loss\", simple_value=train_loss),\n",
    "                tf.compat.v1.Summary.Value(tag=\"train_auc\", simple_value=train_auc),\n",
    "                tf.compat.v1.Summary.Value(tag=\"train_accuracy\", simple_value=train_accuracy),\n",
    "                tf.compat.v1.Summary.Value(tag=\"valid_loss\", simple_value=valid_loss),\n",
    "                tf.compat.v1.Summary.Value(tag=\"valid_auc\", simple_value=valid_auc),\n",
    "                tf.compat.v1.Summary.Value(tag=\"valid_accuracy\", simple_value=valid_accuracy),\n",
    "            ]\n",
    "        )\n",
    "        model.tensorboard_writer.add_summary(tf_summary, epoch)\n",
    "        \n",
    "        # save the model if the loss is lower\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            best_acc = valid_accuracy\n",
    "            best_auc = valid_auc\n",
    "            best_epoch = epoch+1\n",
    "\n",
    "            if args.save:\n",
    "                model_dir = \"ep{:03d}-auc{:.0f}-acc{:.0f}\".format(\n",
    "                    epoch+1, valid_auc*100, valid_accuracy*100,\n",
    "                )\n",
    "                model_name = \"Deep-IRT\"\n",
    "                save_path = os.path.join(args.checkpoint_dir, model_dir, model_name)\n",
    "                saver.save(sess=model.sess, save_path=save_path)\n",
    "\n",
    "                logger.info(\"Model improved. Save model to {}\".format(save_path))\n",
    "            else:\n",
    "                logger.info(\"Model improved.\")\n",
    "\n",
    "    # print out the final result\n",
    "    msg = \"Best result at epoch {}: AUC: {:.2f}\\t Accuracy: {:.2f}\\t Loss: {:.4f}\\t Valid ability: {}\".format(\n",
    "        best_epoch, best_auc*100, best_acc*100, best_loss, valid_ability\n",
    "    )\n",
    "    logger.info(msg)\n",
    "    return best_auc, best_acc, best_loss, valid_ability\n",
    "\n",
    "def cross_validation():\n",
    "    tf.random.set_seed(1234)\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    if args.cpu:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "    aucs, accs, losses = list(), list(), list()\n",
    "    abilities = list()\n",
    "    for i in range(2):\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        logger.info(\"Cross Validation {}\".format(i+1))\n",
    "        result_csv_path = os.path.join(args.result_log_dir, 'fold-{}-result'.format(i)+'.csv')\n",
    "\n",
    "        with tf.compat.v1.Session(config=config) as sess:\n",
    "            data_loader = DataLoader(args.n_questions, args.seq_len, ',')\n",
    "            model = DeepIRTModel(args, sess, name=\"Deep-IRT\")\n",
    "            sess.run(tf.compat.v1.global_variables_initializer())\n",
    "            if args.train:\n",
    "                train_data_path = os.path.join(args.data_dir, args.data_name+'_train{}.csv'.format(i))\n",
    "                valid_data_path = os.path.join(args.data_dir, args.data_name+'_valid{}.csv'.format(i))\n",
    "                logger.info(\"Reading {} and {}\".format(train_data_path, valid_data_path))\n",
    "\n",
    "                train_q_data, train_qa_data = data_loader.load_data(train_data_path)\n",
    "                valid_q_data, valid_qa_data = data_loader.load_data(valid_data_path)\n",
    "\n",
    "                auc, acc, loss, ability = train(\n",
    "                    model, \n",
    "                    train_q_data, train_qa_data, \n",
    "                    valid_q_data, valid_qa_data, \n",
    "                    result_log_path=result_csv_path,\n",
    "                    args=args\n",
    "                )\n",
    "\n",
    "                aucs.append(auc)\n",
    "                accs.append(acc)\n",
    "                losses.append(loss)\n",
    "                abilities.append(ability)\n",
    "                \n",
    "    cross_validation_msg = \"Cross Validation Result:\\n\"\n",
    "    cross_validation_msg += \"AUC: {:.2f} +/- {:.2f}\\n\".format(np.average(aucs)*100, np.std(aucs)*100)\n",
    "    cross_validation_msg += \"Accuracy: {:.2f} +/- {:.2f}\\n\".format(np.average(accs)*100, np.std(accs)*100)\n",
    "    cross_validation_msg += \"Loss: {:.2f} +/- {:.2f}\\n\".format(np.average(losses), np.std(losses))\n",
    "    logger.info(cross_validation_msg)\n",
    "\n",
    "    # write result\n",
    "    result_msg = datetime.datetime.now().strftime(\"%Y-%m-%dT%H%M\") + ','\n",
    "    result_msg += str(args.dataset) + ','\n",
    "    result_msg += str(args.memory_size) + ','\n",
    "    result_msg += str(args.key_memory_state_dim) + ','\n",
    "    result_msg += str(args.value_memory_state_dim) + ','\n",
    "    result_msg += str(args.summary_vector_output_dim) + ','\n",
    "    result_msg += str(np.average(aucs)*100) + ','\n",
    "    result_msg += str(np.std(aucs)*100) + ','\n",
    "    result_msg += str(np.average(accs)*100) + ','\n",
    "    result_msg += str(np.std(accs)*100) + ','\n",
    "    result_msg += str(np.average(losses)) + ','\n",
    "    result_msg += str(np.std(losses)) + '\\n'\n",
    "    with open('Model/results/all_result.csv', 'a') as f:\n",
    "        f.write(result_msg)\n",
    "    \n",
    "    return abilities\n",
    "if __name__=='__main__':\n",
    "    abilities = cross_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f355415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.17799792,  0.09599354,  0.0332391 ,  0.04857728, -0.01726536,\n",
      "       -0.04803391,  0.08205722, -0.01927519], dtype=float32), array([-0.18632653, -0.06553315, -0.18330035,  0.05850279, -0.08913452,\n",
      "       -0.02749979,  0.00800037, -0.00443278], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(abilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfcc461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2d7a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analytics",
   "language": "python",
   "name": "data_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
